
def tokenizador(file)
    file = open(file, "r")
    lines = [line.strip() for line in file.readlines() if line.strip() != ""]
    file.close()
    expr = ''.join(lines)
    pattern = r"while|[(a-z0-9<>){}]|=="
    tokenArray = re.finditer(pattern, expr)
    tokenizador = []
    for tokens in tokenArray:
        token = tokens.group()
        tokenizador.append(token)
    return tokenizdor
